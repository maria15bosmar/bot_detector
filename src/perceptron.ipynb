{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os, sys\n",
    "PATH = os.path.abspath(\"\")[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[340, False, 26626, 97, 19968, 134, 6255, 4.637254901960785, 17.470588235294116, 271.45098039215685, 0.8921568627450981, 0.1568627450980392, 26529]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maria\\Documents\\Uni\\ia orgs\\bot_detector\\.venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SCRAPPEAR UN USUARIO\n",
    "from test_user import get_user_data\n",
    "\n",
    "USER = \"ComicGirlAshley\"\n",
    "user_data = get_user_data(USER)\n",
    "print(user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   factor_rep  verified  followersCount  friendsCount  tweetsCount  \\\n",
      "0         205         0            2346          3848         1894   \n",
      "1         263         0              60           199           52   \n",
      "2          42         0             179          3189         3949   \n",
      "3         321         0             121          1050           12   \n",
      "4        6885         0              26             1         3160   \n",
      "\n",
      "   listedCount  mediaCount  avg_reply  avg_retweet  like_reply  avg_quote  \\\n",
      "0           11         335   0.422222     0.000000    0.711111      0.000   \n",
      "1            0           2   0.650000     0.100000    2.925000      0.075   \n",
      "2            4          55   0.121951     0.073171    0.731707      0.000   \n",
      "3            0           1   0.000000     0.000000    0.571429      0.000   \n",
      "4            8        3159   0.000000     0.019231    0.000000      0.000   \n",
      "\n",
      "   hora_rep  followers_diff  clase  \n",
      "0  0.222222           -2346    bot  \n",
      "1  0.275000             -60    bot  \n",
      "2  0.097561            -179  human  \n",
      "3  0.428571            -121    bot  \n",
      "4  0.192308             -26    bot  \n"
     ]
    }
   ],
   "source": [
    "# Se leen los datos generados con model.py\\n\",\n",
    "df_data = pd.read_csv(PATH + \"data/training_new.csv\").sample(frac=1).reset_index(drop=True)\n",
    "data = df_data.iloc[:, 1:]\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "header_names = list(data.columns.values)\n",
    "for col in data.iloc[:,:-1]:\n",
    "    data[col] = preprocessing.MinMaxScaler().fit_transform(np.array(data[col]).reshape(-1,1))\n",
    "x_data = data.iloc[:,:-1].to_numpy()\n",
    "y_data = data.iloc[:,-1].to_numpy()\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_data, y_data, stratify=y_data, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human' 'bot' 'bot' 'human' 'human']\n",
      "[[1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "#CONVERTIRLA SALIDA A UN VECTOR DE 0 Y 1\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "encoder=LabelBinarizer()\n",
    "label = encoder.fit_transform(y_train)\n",
    "y_train_transformed = np.hstack((label, 1 - label))\n",
    "label = encoder.fit_transform(y_test)\n",
    "y_test_transformed = np.hstack((label, 1 - label))\n",
    "print(y_train[:5])\n",
    "print(y_train_transformed[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(1616, 13)\n",
      "(1616,)\n",
      "(1616, 2)\n",
      "(796, 13)\n",
      "(796,)\n",
      "(796, 2)\n"
     ]
    }
   ],
   "source": [
    "# COMPROBAR TIPO DE LOS DATOS\n",
    "print(type(x_train))\n",
    "print(type(y_train_transformed))\n",
    "# COMPROBAR DIMENSIONES DE LOS DATOS\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train_transformed.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(y_test_transformed.shape) \n",
    "# OBTENER DIMENSION DE LA ENTRADA Y NÃšMERO DE SALIDAS\n",
    "input_shape = (x_train.shape[1] ,)\n",
    "num_clases = y_test_transformed.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_67 (Dense)            (None, 20)                280       \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 2)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 742\n",
      "Trainable params: 742\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Crear modelo usando sigmoide\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "input_shape= (x_train.shape[1] ,)\n",
    "model = Sequential()\n",
    "arch = [20, 20, 0, \"sigmoid\", 0.2]\n",
    "model.add(Dense(arch[0], input_shape=input_shape, activation=arch[3]))\n",
    "model.add(Dense(arch[1], input_shape=input_shape, activation=arch[3]))\n",
    "#model.add(Dense(arch[2], input_shape=input_shape, activation=arch[3]))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80000\n",
      "81/81 [==============================] - 1s 3ms/step - loss: 0.2213 - accuracy: 0.6618 - mse: 0.2213 - val_loss: 0.2005 - val_accuracy: 0.7130 - val_mse: 0.2005\n",
      "Epoch 2/80000\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.1970 - accuracy: 0.6974 - mse: 0.1970 - val_loss: 0.1949 - val_accuracy: 0.7191 - val_mse: 0.1949\n",
      "Epoch 3/80000\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.1945 - accuracy: 0.6920 - mse: 0.1945 - val_loss: 0.1940 - val_accuracy: 0.6944 - val_mse: 0.1940\n",
      "Epoch 4/80000\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.1938 - accuracy: 0.6889 - mse: 0.1938 - val_loss: 0.1932 - val_accuracy: 0.6944 - val_mse: 0.1932\n",
      "Epoch 5/80000\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.1934 - accuracy: 0.6865 - mse: 0.1934 - val_loss: 0.1926 - val_accuracy: 0.6852 - val_mse: 0.1926\n",
      "Epoch 6/80000\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.1931 - accuracy: 0.6858 - mse: 0.1931 - val_loss: 0.1918 - val_accuracy: 0.6914 - val_mse: 0.1918\n",
      "Epoch 7/80000\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.1929 - accuracy: 0.6842 - mse: 0.1929 - val_loss: 0.1912 - val_accuracy: 0.6914 - val_mse: 0.1912\n",
      "Epoch 8/80000\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.1927 - accuracy: 0.6873 - mse: 0.1927 - val_loss: 0.1909 - val_accuracy: 0.6883 - val_mse: 0.1909\n",
      "Epoch 9/80000\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.1925 - accuracy: 0.6889 - mse: 0.1925 - val_loss: 0.1904 - val_accuracy: 0.6914 - val_mse: 0.1904\n",
      "Epoch 10/80000\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.1924 - accuracy: 0.6889 - mse: 0.1924 - val_loss: 0.1900 - val_accuracy: 0.6914 - val_mse: 0.1900\n",
      "Epoch 11/80000\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.1923 - accuracy: 0.6889 - mse: 0.1923 - val_loss: 0.1895 - val_accuracy: 0.7037 - val_mse: 0.1895\n",
      "Epoch 12/80000\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.1920 - accuracy: 0.6842 - mse: 0.1920 - val_loss: 0.1892 - val_accuracy: 0.6914 - val_mse: 0.1892\n",
      "Epoch 13/80000\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.1919 - accuracy: 0.6842 - mse: 0.1919 - val_loss: 0.1890 - val_accuracy: 0.6944 - val_mse: 0.1890\n",
      "Epoch 14/80000\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.1918 - accuracy: 0.6858 - mse: 0.1918 - val_loss: 0.1886 - val_accuracy: 0.6944 - val_mse: 0.1886\n",
      "Epoch 15/80000\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.1917 - accuracy: 0.6842 - mse: 0.1917 - val_loss: 0.1884 - val_accuracy: 0.6944 - val_mse: 0.1884\n",
      "Epoch 16/80000\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.1916 - accuracy: 0.6850 - mse: 0.1916 - val_loss: 0.1882 - val_accuracy: 0.6914 - val_mse: 0.1882\n",
      "Epoch 17/80000\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.1915 - accuracy: 0.6858 - mse: 0.1915 - val_loss: 0.1879 - val_accuracy: 0.6975 - val_mse: 0.1879\n",
      "Epoch 18/80000\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.1915 - accuracy: 0.6865 - mse: 0.1915 - val_loss: 0.1878 - val_accuracy: 0.7006 - val_mse: 0.1878\n",
      "Epoch 19/80000\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.1913 - accuracy: 0.6873 - mse: 0.1913 - val_loss: 0.1872 - val_accuracy: 0.6944 - val_mse: 0.1872\n",
      "Epoch 20/80000\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.1913 - accuracy: 0.6873 - mse: 0.1913 - val_loss: 0.1874 - val_accuracy: 0.6944 - val_mse: 0.1874\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURAR MODELO Y ENTRENAMIENTO\n",
    "from keras.callbacks import EarlyStopping\n",
    "lr = 0.2\n",
    "arch[4] = lr\n",
    "epochs = 80000\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=1)\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr, momentum=0), metrics=['accuracy','mse'], loss='mean_squared_error')\n",
    "historico = model.fit(x_train, y_train_transformed, epochs=epochs, verbose=1, validation_split=0.2,\n",
    "    shuffle=False, validation_freq=1, batch_size=16, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "0.1913202852010727\n",
      "0.1871906965970993\n",
      "0.6873065233230591\n",
      "0.6944444179534912\n"
     ]
    }
   ],
   "source": [
    "col_names = ['arquitectura', 'loss', 'val_loss', 'accuracy', 'val_accuracy', 'epoch']\n",
    "#dff = pd.DataFrame([], columns=col_names)\n",
    "dff = pd.read_csv(PATH + 'data/vals2.csv')\n",
    "final_epoch=len(historico.history['loss'])-1\n",
    "print(final_epoch)\n",
    "tloss = historico.history['loss'][final_epoch-1]\n",
    "print(tloss)\n",
    "vloss = historico.history['val_loss'][final_epoch-1]\n",
    "print(vloss)\n",
    "tacc = historico.history['accuracy'][final_epoch-1]\n",
    "print(tacc)\n",
    "vacc = historico.history['val_accuracy'][final_epoch-1]\n",
    "print(vacc)\n",
    "datos = pd.DataFrame([[arch, tloss, vloss, tacc, vacc, final_epoch]], columns=col_names)\n",
    "datos = pd.concat([dff, datos], ignore_index=True)\n",
    "datos.to_csv(PATH + 'data/vals2.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8d24b60153e914bfadbc0d5c382c116391f93a7ba6cc3a1d5bcf7201918ae2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
