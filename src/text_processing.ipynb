{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "PATH = os.path.abspath(\"\")[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLO HACER LA PRIMERA VEZ QUE SE TIRA\n",
    "# Se va a abrir en una ventana aparte de python, le das a download todo y cuando acabe la cierras.\n",
    "# Si todo ha salido guay devuelve True.\n",
    "nltk.download()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparar datos para predecir un usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SCRAPPEAR UN USUARIO\n",
    "from scrap import scrap_user\n",
    "\n",
    "USER = \"ComicGirlAshley\"\n",
    "tweets = scrap_user(USER)\n",
    "tweets.to_csv(PATH+\"data/example.csv\", index=False) # Se guardan los tweets en un csv pero no hace falta lol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CÁLCULO DEL FACTOR REP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tweets = pd.read_csv(PATH+\"data/example.csv\") # Se puede leer el csv de antes.\n",
    "contents = tweets[\"content\"]\n",
    "toks = []\n",
    "sw = nltk.corpus.stopwords\n",
    "stemmer = nltk.stem.snowball.EnglishStemmer()\n",
    "# Tokenizar, stemmear, filtrar stop-words.\n",
    "for content in contents.values:\n",
    "    tk = nltk.word_tokenize(content)\n",
    "    filtered_sentence = [w for w in tk if not w.lower() in sw.words('english')]\n",
    "    toks.append([stemmer.stem(word) for word in filtered_sentence])\n",
    "# Se cuenta número de repeticiones de palabras y se suman.\n",
    "try:\n",
    "    vect = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False, ngram_range=(1, 15), min_df=0.1).fit_transform(toks)\n",
    "    factor_rep = vect.sum()\n",
    "except:\n",
    "    factor_rep = 0\n",
    "print(factor_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPETICIÓN DE LAS FECHAS\n",
    "from datetime import datetime\n",
    "date = tweets[\"date\"]\n",
    "hours = []\n",
    "for h in date.values:\n",
    "    hours.append(pd.to_datetime(h).hour)\n",
    "valores_finales=[]\n",
    "tot= len(hours)\n",
    "result = dict()\n",
    "#contando el numero de repeticiones\n",
    "for j in hours:\n",
    "    if j not in result:\n",
    "        result[j] = 0 \n",
    "    result[j] += 1\n",
    "\n",
    "#viendo el mayor valor y computando porcentaje\n",
    "values=[]\n",
    "mayor=0\n",
    "for dato, valor in result.items():\n",
    "    if valor > mayor:\n",
    "        mayor= valor\n",
    "hora_rep = mayor/tot\n",
    "print(hora_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS MEDIOS\n",
    "avg_likes = np.average(tweets[\"likeCount\"].values)\n",
    "avg_retweets = np.average(tweets[\"retweetCount\"].values)\n",
    "avg_reply = np.average(tweets[\"replyCount\"].values)\n",
    "avg_quote = np.average(tweets[\"quoteCount\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El usuario listo para predecir se guarda en user_data.\n",
    "user_data = tweets[[\"verified\", \"followersCount\", \"friendsCount\", \"tweetsCount\", \"listedCount\", \"mediaCount\"]].iloc[-1, :].values.tolist()\n",
    "user_data.insert(0, factor_rep)\n",
    "user_data.extend([avg_reply, avg_retweets, avg_likes, avg_quote, hora_rep, user_data[2]-user_data[3]])\n",
    "print(user_data)\n",
    "print(len(user_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparacion train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se leen los datos generados con model.py\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "training = pd.read_csv(PATH+\"data/training_new.csv\").sample(frac=1).reset_index(drop=True)\n",
    "training[\"clase\"].replace(\"bot\",1, inplace= True)\n",
    "training[\"clase\"].replace(\"human\",0, inplace= True)\n",
    "training.pop(\"username\")\n",
    "label = training.pop(\"clase\")\n",
    "#se usaran para el estudio de los hiperparemtros dos funciones de normalización\n",
    "scaler= StandardScaler()\n",
    "data_s = scaler.fit_transform(training)\n",
    "scaler_m= MinMaxScaler()\n",
    "data_m = scaler.fit_transform(training)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entrenamiento de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hiperparametros regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= data_s\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, label)\n",
    "\n",
    "parameters= {\"solver\":[\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\", \"sag\", \"saga\"]}\n",
    "reg= LogisticRegression()\n",
    "\n",
    "reg_n = GridSearchCV(reg, parameters, cv=5)\n",
    "reg_n.fit(x_train, y_train)\n",
    "print(reg_n.best_params_)\n",
    "print(reg_n.best_score_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regresion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= data_s\n",
    "skf= StratifiedKFold(n_splits=4)\n",
    "precision=[]\n",
    "sens=[]\n",
    "spec=[]\n",
    "acu=[]\n",
    "\n",
    "for train_index, test_index in skf.split(data, label):\n",
    "    x_train= data[train_index]\n",
    "    y_train = label[train_index]\n",
    "    x_test= data[test_index]\n",
    "    y_test = label[test_index]\n",
    "\n",
    "    reg = LogisticRegression(solver= \"sag\").fit(x_train, y_train)\n",
    "    y_prep = reg.predict(x_test)\n",
    "    acu.append(reg.score(x_test, y_test))\n",
    "    tn, fp, fn, tp= metrics.confusion_matrix(y_test, y_prep).ravel()\n",
    "    precision.append(tp/(tp+fp))\n",
    "    sens.append(tp/(tp+fn))\n",
    "    spec.append(tn/(tn+fp))\n",
    "\n",
    "print(\"acurracy\", np.mean(acu))\n",
    "print(\"precision\" ,np.mean(precision))\n",
    "print(\"sens\", np.mean(sens))\n",
    "print(\"spec\", np.mean(spec))\n",
    "\n",
    "'''\n",
    "acurracy 0.7101990049751243\n",
    "precision 0.6736761164619097\n",
    "sens 0.9750772953897953\n",
    "spec 0.34062954531082024'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hiperparametros xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= data_m\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, label)\n",
    "\n",
    "parameters= {\"booster\": [\"gbtree\", \"gblinear\", \"dart\"], \"nthread\": range(1,32,2)}\n",
    "xgb_= xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "\n",
    "xgb_n = GridSearchCV(xgb_, parameters, cv=5)\n",
    "xgb_n.fit(x_train, y_train)\n",
    "print(xgb_n.best_params_)\n",
    "print(xgb_n.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= data_m\n",
    "skf= StratifiedKFold(n_splits=4)\n",
    "precision=[]\n",
    "sens=[]\n",
    "spec=[]\n",
    "acu=[]\n",
    "\n",
    "for train_index, test_index in skf.split(data, label):\n",
    "    x_train= data[train_index]\n",
    "    y_train = label[train_index]\n",
    "    x_test= data[test_index]\n",
    "    y_test = label[test_index]\n",
    "\n",
    "    xgb_= xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", booster= \"gbtree\", nthread= 1).fit(x_train,y_train)\n",
    "    y_prep = xgb_.predict(x_test)\n",
    "    acu.append(xgb_.score(x_test, y_test))\n",
    "    tn, fp, fn, tp= metrics.confusion_matrix(y_test, y_prep).ravel()\n",
    "    precision.append(tp/(tp+fp))\n",
    "    sens.append(tp/(tp+fn))\n",
    "    spec.append(tn/(tn+fp))\n",
    "\n",
    "print(\"acurracy\", np.mean(acu))\n",
    "print(\"precision\" ,np.mean(precision))\n",
    "print(\"sens\", np.mean(sens))\n",
    "print(\"spec\", np.mean(spec))\n",
    "\n",
    "'''\n",
    "acurracy 0.8146766169154229\n",
    "precision 0.8303567912649471\n",
    "sens 0.8569367553742553\n",
    "spec 0.7556994245241258\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hiperparametro neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= data_m\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, label)\n",
    "\n",
    "parameters= {\"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"], \"learning_rate\":[\"constant,invscaling\", \"adaptive\"]}\n",
    "ne= MLPClassifier(max_iter=10000)\n",
    "\n",
    "ne_n = GridSearchCV(ne, parameters, cv=5)\n",
    "ne_n.fit(x_train, y_train)\n",
    "print(ne_n.best_params_)\n",
    "print(ne_n.best_score_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= data_m\n",
    "skf= StratifiedKFold(n_splits=4)\n",
    "precision=[]\n",
    "sens=[]\n",
    "spec=[]\n",
    "acu=[]\n",
    "\n",
    "for train_index, test_index in skf.split(data, label):\n",
    "    x_train= data[train_index]\n",
    "    y_train = label[train_index]\n",
    "    x_test= data[test_index]\n",
    "    y_test = label[test_index]\n",
    "\n",
    "    ne = MLPClassifier(activation=\"relu\",learning_rate=\"adaptive\", max_iter=10000).fit(x_train,y_train)\n",
    "    y_prep = ne.predict(x_test)\n",
    "    acu.append(ne.score(x_test, y_test))\n",
    "    tn, fp, fn, tp= metrics.confusion_matrix(y_test, y_prep).ravel()\n",
    "    precision.append(tp/(tp+fp))\n",
    "    sens.append(tp/(tp+fn))\n",
    "    spec.append(tn/(tn+fp))\n",
    "\n",
    "print(\"acurracy\", np.mean(acu))\n",
    "print(\"precision\" ,np.mean(precision))\n",
    "print(\"sens\", np.mean(sens))\n",
    "print(\"spec\", np.mean(spec))\n",
    "\n",
    "'''\n",
    "acurracy 0.7645107794361525\n",
    "precision 0.7549646432499513\n",
    "sens 0.8832718207718208\n",
    "spec 0.5988387718965409\n",
    "\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "probar usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformando los datos del scrapeado a la normalizacion necesaria y se selecciona el modelo querido\n",
    "\n",
    "model= xgb_\n",
    "scaler_m= MinMaxScaler()\n",
    "user_d=np.array(user_data)\n",
    "user_array= user_d.reshape(-1, 1)\n",
    "user_array = scaler.fit_transform(user_array)\n",
    "\n",
    "final= []\n",
    "for i in range(0,13):\n",
    "    final.append(float(user_array[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime la probabilidad de que el usuario preparado antes sea bot.\n",
    "print(model.predict_proba(np.array([final]))[0])\n",
    "result= model.predict(np.array([final]))[0]\n",
    "if result==1:\n",
    "    print(\"bot\")\n",
    "else:\n",
    "    print(\"human\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9f91b089cef86f9556c03564b23a080d62551fde720f42de76517327a4e0d44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
