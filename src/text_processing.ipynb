{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "PATH = os.path.abspath(\"\")[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLO HACER LA PRIMERA VEZ QUE SE TIRA\n",
    "# Se va a abrir en una ventana aparte de python, le das a download todo y cuando acabe la cierras.\n",
    "# Si todo ha salido guay devuelve True.\n",
    "nltk.download()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparar datos para predecir un usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SCRAPPEAR UN USUARIO\n",
    "from scrap import scrap_user\n",
    "\n",
    "USER = \"ComicGirlAshley\"\n",
    "tweets = scrap_user(USER)\n",
    "tweets.to_csv(PATH+\"data/example.csv\", index=False) # Se guardan los tweets en un csv pero no hace falta lol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maria\\Documents\\Uni\\ia orgs\\bot_detector\\.venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# CÁLCULO DEL FACTOR REP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#tweets = pd.read_csv(PATH+\"data/example.csv\") # Se puede leer el csv de antes.\n",
    "contents = tweets[\"content\"]\n",
    "toks = []\n",
    "sw = nltk.corpus.stopwords\n",
    "stemmer = nltk.stem.snowball.EnglishStemmer()\n",
    "# Tokenizar, stemmear, filtrar stop-words.\n",
    "for content in contents.values:\n",
    "    tk = nltk.word_tokenize(content)\n",
    "    filtered_sentence = [w for w in tk if not w.lower() in sw.words('english')]\n",
    "    toks.append([stemmer.stem(word) for word in filtered_sentence])\n",
    "# Se cuenta número de repeticiones de palabras y se suman.\n",
    "try:\n",
    "    vect = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False, ngram_range=(1, 15), min_df=0.1).fit_transform(toks)\n",
    "    factor_rep = vect.sum()\n",
    "except:\n",
    "    factor_rep = 0\n",
    "print(factor_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1568627450980392\n"
     ]
    }
   ],
   "source": [
    "# REPETICIÓN DE LAS FECHAS\n",
    "from datetime import datetime\n",
    "date = tweets[\"date\"]\n",
    "hours = []\n",
    "for h in date.values:\n",
    "    hours.append(pd.to_datetime(h).hour)\n",
    "valores_finales=[]\n",
    "tot= len(hours)\n",
    "result = dict()\n",
    "#contando el numero de repeticiones\n",
    "for j in hours:\n",
    "    if j not in result:\n",
    "        result[j] = 0 \n",
    "    result[j] += 1\n",
    "\n",
    "#viendo el mayor valor y computando porcentaje\n",
    "values=[]\n",
    "mayor=0\n",
    "for dato, valor in result.items():\n",
    "    if valor > mayor:\n",
    "        mayor= valor\n",
    "hora_rep = mayor/tot\n",
    "print(hora_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS MEDIOS\n",
    "avg_likes = np.average(tweets[\"likeCount\"].values)\n",
    "avg_retweets = np.average(tweets[\"retweetCount\"].values)\n",
    "avg_reply = np.average(tweets[\"replyCount\"].values)\n",
    "avg_quote = np.average(tweets[\"quoteCount\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[340, False, 26626, 97, 19968, 134, 6255, 4.637254901960785, 17.470588235294116, 271.45098039215685, 0.8921568627450981, 0.1568627450980392, 26529]\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# El usuario listo para predecir se guarda en user_data.\n",
    "user_data = tweets[[\"verified\", \"followersCount\", \"friendsCount\", \"tweetsCount\", \"listedCount\", \"mediaCount\"]].iloc[-1, :].values.tolist()\n",
    "user_data.insert(0, factor_rep)\n",
    "user_data.extend([avg_reply, avg_retweets, avg_likes, avg_quote, hora_rep, user_data[2]-user_data[3]])\n",
    "print(user_data)\n",
    "print(len(user_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   factor_rep  verified  followersCount  friendsCount  tweetsCount  \\\n",
      "0         204         0              34           356          457   \n",
      "1         208         0          364588        359855        30978   \n",
      "2         206         0            7719          2678       231028   \n",
      "3         182         0              23            50         2968   \n",
      "4         225         0              48           222          588   \n",
      "\n",
      "   listedCount  mediaCount  avg_reply  avg_retweet  like_reply  avg_quote  \\\n",
      "0            1          18   0.187500     0.020833    0.583333   0.000000   \n",
      "1         3997        6244   0.977273     1.204545   11.431818   0.068182   \n",
      "2          431       28121   0.479167     0.291667    5.500000   0.062500   \n",
      "3            1           0   0.600000     0.000000    0.200000   0.000000   \n",
      "4            1          35   0.342857     0.028571    1.914286   0.000000   \n",
      "\n",
      "   hora_rep  followers_diff  \n",
      "0  0.270833             -34  \n",
      "1  0.136364         -364588  \n",
      "2  0.166667           -7719  \n",
      "3  0.200000             -23  \n",
      "4  0.171429             -48  \n",
      "   clase\n",
      "0    bot\n",
      "1  human\n",
      "2  human\n",
      "3    bot\n",
      "4  human\n"
     ]
    }
   ],
   "source": [
    "# Se leen los datos generados con model.py\n",
    "training = pd.read_csv(PATH+\"data/training_new.csv\").sample(frac=1).reset_index(drop=True)\n",
    "x_train = training.iloc[:, 1:-1]\n",
    "y_train = training.iloc[:, -1:]\n",
    "print(x_train.head())\n",
    "print(y_train.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresión logística, aquí habría que probar perceptrones y demás\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression().fit(x_train, np.array(y_train.values.T[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47215778 0.52784222]\n",
      "human\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maria\\Documents\\Uni\\ia orgs\\bot_detector\\.venv\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\maria\\Documents\\Uni\\ia orgs\\bot_detector\\.venv\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Imprime la probabilidad de que el usuario preparado antes sea bot.\n",
    "print(model.predict_proba(np.array([user_data]))[0])\n",
    "print(model.predict(np.array([user_data]))[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8d24b60153e914bfadbc0d5c382c116391f93a7ba6cc3a1d5bcf7201918ae2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
